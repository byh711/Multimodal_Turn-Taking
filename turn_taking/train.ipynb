{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and frameworks\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets for different modalities (text, audio, vision, and game) for training and testing\n",
    "\n",
    "train_target = np.load('path_to_data/train_target.npy')\n",
    "train_text = np.load('path_to_data/train_text.npy')\n",
    "train_audio = np.load('path_to_data/train_audio.npy')\n",
    "train_vision = np.load('path_to_data/train_vision.npy')\n",
    "train_game = np.load('path_to_data/train_game.npy')\n",
    "\n",
    "test_target = np.load('path_to_data/test_target.npy')\n",
    "test_text = np.load('path_to_data/test_text.npy')\n",
    "test_audio = np.load('path_to_data/test_audio.npy')\n",
    "test_vision = np.load('path_to_data/test_vision.npy')\n",
    "test_game = np.load('path_to_data/test_game.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for training\n",
    "# This includes caching, shuffling, batching, and prefetching to optimize data loading during training\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'text_input': train_text,\n",
    "        'audio_input': train_audio,\n",
    "        'vision_input': train_vision,\n",
    "        'game_input': train_game\n",
    "    },\n",
    "    {\n",
    "        'outputs': train_target  \n",
    "    },\n",
    "))\n",
    "\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare  dataset for testing\n",
    "# This includes caching, batching, and prefetching to optimize data loading during evaluation\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'text_input': test_text,\n",
    "        'audio_input': test_audio,\n",
    "        'vision_input': test_vision,\n",
    "        'game_input': test_game\n",
    "    },\n",
    "    {\n",
    "        'outputs': test_target  \n",
    "    },\n",
    "))\n",
    "\n",
    "test_dataset = test_dataset.cache()\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for the model and training process\n",
    "\n",
    "text_size = train_text.shape[1]           # Text Vector length\n",
    "audio_size = train_audio.shape[1]         # Audio Vector length\n",
    "vision_size = train_vision.shape[1]       # Vision Vector length\n",
    "game_size = train_game.shape[1]           # Game Vector length\n",
    "\n",
    "\n",
    "text_dim = train_text.shape[2]\n",
    "audio_dim = train_audio.shape[2]\n",
    "vision_dim = train_vision.shape[2]\n",
    "game_dim = train_game.shape[2]\n",
    "\n",
    "\n",
    "kernel_size = 1\n",
    "num_layers = 3        # Number of layers in the encoder and decoder of the model\n",
    "dff = 128             # Hidden layer size in feed-forward network\n",
    "d_model = 32          # Input and output size of the transformer's encoder and decoder\n",
    "num_heads = 8         # Number of parallel attention heads\n",
    "dropout = 0.1         # Dropout rate\n",
    "\n",
    "lr_rate = 0.00005     # Learning rate\n",
    "neg, pos = np.bincount(train_target.astype(np.int64))\n",
    "total = neg + pos\n",
    "output_bias = tf.keras.initializers.Constant(np.log([pos/neg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89afc7ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from Working_code.model_architecture import overall\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "model = overall(\n",
    "    text_size = text_size,\n",
    "    audio_size = audio_size,\n",
    "    vision_size = vision_size,\n",
    "    game_size = game_size,\n",
    "    text_dim = text_dim,\n",
    "    audio_dim = audio_dim,\n",
    "    vision_dim = vision_dim,\n",
    "    game_dim = game_dim,\n",
    "    kernel_size = kernel_size,\n",
    "    num_layers = num_layers,\n",
    "    dff = dff,\n",
    "    d_model=d_model,\n",
    "    num_heads = num_heads,\n",
    "    dropout = dropout,\n",
    "    output_bias = output_bias)\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.AUC(name='auc')\n",
    "]\n",
    "\n",
    "class_weight = class_weight.compute_class_weight('balanced', classes=np.unique(train_target), \n",
    "                                                  y=train_target)\n",
    "\n",
    "focal_loss = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    apply_class_balancing=True, alpha=(class_weight[1]/class_weight[0]), gamma=2)\n",
    "\n",
    "model.compile(loss=focal_loss, \n",
    "              optimizer=tf.keras.optimizers.Adam(lr_rate), metrics=METRICS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2512da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training and validation accuracy and loss over epochs\n",
    "# Note: Validation curves are generally smoother than training curves.\n",
    "# This is mainly because dropout layers are not activated during evaluation.\n",
    "\n",
    "name = 'ours'\n",
    "checkpoint_path = \"saved_model/\" + types + \".h5\"\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_accuracy', \n",
    "                                                   mode='max', save_best_only=True, save_weights_only=True)\n",
    "                                                  \n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss', mode='min')\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(patience=2, monitor='val_loss', factor=0.5)\n",
    "\n",
    "history = model.fit(train_dataset, epochs=100,\n",
    "                    validation_data = test_dataset,\n",
    "                    callbacks = [checkpoint_cb, early_stopping_cb, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d680e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test dataset\n",
    "# Predict the test dataset\n",
    "# Evaluate the predictions using metrics like accuracy, F1 score, and AUC\n",
    "\n",
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "plt.plot(epochs, history.history['accuracy'])\n",
    "plt.plot(epochs, history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'valid'], loc='lower right')\n",
    "plt.ylim([0.7, 1])   \n",
    "plt.show()\n",
    "\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "plt.plot(epochs, history.history['loss'])\n",
    "plt.plot(epochs, history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
